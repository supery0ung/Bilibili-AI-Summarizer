# Bilibili Summarizer æµç¨‹è¯´æ˜ (V3 Batch Edition)

> å°† Bç«™"ç¨åå†çœ‹"è§†é¢‘è‡ªåŠ¨è½¬å½•ä¸ºæ–‡å­—ï¼Œç”¨ Qwen3 æ ¡æ­£+æ€»ç»“ï¼Œç”Ÿæˆ EPUB ç”µå­ä¹¦ï¼Œä¸Šä¼ å¾®ä¿¡è¯»ä¹¦ã€‚
> **V3 ç‰¹æ€§**ï¼šé‡‡ç”¨æ‰¹é‡å¤„ç†ç®¡é“ï¼ˆBatch Pipelineï¼‰ï¼Œæ”¯æŒ 12GB æ˜¾å¡çš„æ˜¾å­˜è‡ªåŠ¨å›æ”¶ä¸æ–­ç‚¹ç»­ä¼ ã€‚

## ğŸ“Š æ‰¹é‡å¤„ç†ç®¡é“æ¶æ„

ä¸ºäº†ä¼˜åŒ–æ˜¾å­˜å ç”¨ï¼Œæ•´ä¸ªæµç¨‹åˆ†ä¸ºå››ä¸ªä¸»è¦çš„**æ‰¹é‡é˜¶æ®µ**ï¼Œé˜¶æ®µé—´ä¼šè‡ªåŠ¨é‡Šæ”¾ GPU èµ„æºï¼š

```mermaid
graph TD
    A[Step A: Fetch] -->|è·å–åˆ—è¡¨| B(Step B: Batch Download)
    B -->|ä¸‹è½½å®Œæ¯•| C(Step C: Batch Transcribe)
    C -->|é‡Šæ”¾ ASR æ˜¾å­˜| D(Step D&E: Batch LLM)
    D -->|é‡Šæ”¾ LLM æ˜¾å­˜| E(Step F: Batch EPUB)
    E -->|ç”Ÿæˆç”µå­ä¹¦| F(Step G: Batch Upload)
    
    subgraph "ASR é˜¶æ®µ (GPU)"
    C
    end
    
    subgraph "LLM é˜¶æ®µ (GPU/API)"
    D
    end
```

---

## ğŸ”„ å‘½ä»¤è¯´æ˜

### Step A: è·å–è§†é¢‘åˆ—è¡¨
```powershell
python main.py fetch
```

### Step B: ä¸‹è½½éŸ³é¢‘
```powershell
python main.py download --max-items 5
```

### Step BA: AI æ™ºèƒ½è¿‡æ»¤ (Qwen)
- è‡ªåŠ¨è¿è¡Œï¼š`python main.py run`
- é€»è¾‘ï¼šæ ¹æ®æ ‡é¢˜å’Œ UP ä¸»åˆ¤æ–­æ˜¯å¦å€¼å¾—è½¬å½•ã€‚
- è¿‡æ»¤ï¼šæ¸¸æˆã€å½±è§†è§£è¯´ã€ç¡¬æ ¸ä»£ç æ•™ç¨‹ç­‰ã€‚
- ä¿ç•™ï¼šAI æ–°é—»ã€ç§‘æ™®ã€è®¿è°ˆã€æ’­å®¢ç­‰ã€‚

### Step C: è¯­éŸ³è½¬æ–‡å­— (Whisper)
```powershell
python main.py transcribe --max-items 5 --whisper-model large-v3
```
â†’ è¾“å‡º: `{æ ‡é¢˜}.md`

### Step D: æ ¡æ­£æ–‡æœ¬ (Qwen3)
```powershell
python main.py correct --max-items 5
```
- ä½¿ç”¨ `prompts/correct.txt`
- é€æ®µæ ¡æ­£é”™åˆ«å­—å’Œä¹±ç 
â†’ è¾“å‡º: `{æ ‡é¢˜}.corrected.md`

### Step E: ç”Ÿæˆæ‘˜è¦ (Qwen3)
```powershell
python main.py summarize --max-items 5
```
- ä½¿ç”¨ `prompts/summarize.txt`
- ç”Ÿæˆå†…å®¹æ‘˜è¦ + è¦ç‚¹åˆ—è¡¨
â†’ è¾“å‡º: `{æ ‡é¢˜}.final.md` (æ‘˜è¦ + æ ¡æ­£åå…¨æ–‡)

### Step F: ç”Ÿæˆ EPUB
```powershell
python main.py epub
```

### Step G: ä¸Šä¼ å¾®ä¿¡è¯»ä¹¦
```powershell
python main.py upload --max-items 5
```

---

## âš¡ å¿«æ·å‘½ä»¤

```powershell
# æŸ¥çœ‹çŠ¶æ€
python main.py status

# ä¸€é”®è¿è¡Œå…¨éƒ¨ (A â†’ F)
python main.py run --max-items 10 --whisper-model large-v3
```

---

## ğŸ“ è¾“å‡ºæ–‡ä»¶

| æ­¥éª¤ | æ–‡ä»¶ | è¯´æ˜ |
|------|------|------|
| Step C | `{æ ‡é¢˜}.md` | Whisper åŸå§‹è½¬å½• |
| Step D | `{æ ‡é¢˜}.corrected.md` | æ ¡æ­£åçš„æ–‡æœ¬ |
| Step E | `{æ ‡é¢˜}.final.md` | æ‘˜è¦ + æ ¡æ­£æ–‡æœ¬ |
| Step F | `{æ ‡é¢˜}.epub` | ç”µå­ä¹¦ |

---

## ğŸ”§ Prompt è‡ªå®šä¹‰

| æ–‡ä»¶ | ç”¨é€” |
|------|------|
| `prompts/correct.txt` | æ–‡æœ¬æ ¡æ­£ prompt |
| `prompts/summarize.txt` | å†…å®¹æ€»ç»“ prompt |

**å˜é‡è¯´æ˜**ï¼š
- `{text}` - åŸæ–‡/è½¬å½•æ–‡æœ¬
- `{title}` - è§†é¢‘æ ‡é¢˜
- `{author}` - UPä¸»åç§°

---

## ğŸ¤– æ¨¡å‹é…ç½®

`config.yaml`:

```yaml
# Whisper è¯­éŸ³è¯†åˆ«
whisper:
  model: "large-v3"
  language: null  # è‡ªåŠ¨æ£€æµ‹

# Ollama LLM (Qwen3)
ollama:
  model: "qwen3:8b"
  base_url: "http://localhost:11434"
```

**æ¨¡å‹å­˜å‚¨ä½ç½®**ï¼š
- Whisper: `E:/ai_models/whisper/`
- Ollama: `E:/ai_models/ollama/`
- HuggingFace: `E:/ai_models/huggingface/`
